{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46c0f10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8c335db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DSML\\\\Condition2Cure\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d6f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188140e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Files\\\\DSML\\\\Condition2Cure'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a015b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score,\n",
    "    f1_score, confusion_matrix\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from Condition2Cure.utils.helpers import *\n",
    "import dagshub\n",
    "from Condition2Cure import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8709c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Condition2Cure.utils.helpers import *\n",
    "from Condition2Cure.constants import *\n",
    "from Condition2Cure.utils.execptions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be809191",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    features_path: Path\n",
    "    labels_path: Path\n",
    "    model_path: Path\n",
    "    label_encoder_path: Path\n",
    "    test_size: float\n",
    "    random_state: int\n",
    "    metrics_path: Path\n",
    "    cm_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b670a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                 config_filepath = CONFIG_FILE_PATH,\n",
    "                 params_filepath = PARAMS_FILE_PATH,\n",
    "                 schema_filepath = SCHEMA_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        split = self.params.train_test_split\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            features_path=config.features_path,\n",
    "            labels_path=config.labels_path,\n",
    "            model_path=config.model_path,\n",
    "            label_encoder_path=config.label_encoder_path,\n",
    "            test_size=split.test_size,\n",
    "            random_state=split.random_state,\n",
    "            metrics_path=config.metrics_path,\n",
    "            cm_path=config.cm_path\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1feeaeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelEvaluator:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "\n",
    "        mlflow.set_tracking_uri(\"./mlruns\")\n",
    "        mlflow.set_experiment(\"Condition2Cure\")\n",
    "        logger.info(\"MLflow tracking setup complete.\")\n",
    "\n",
    "    def evaluation(self):\n",
    "        # Load data and artifacts\n",
    "        if not os.path.exists(self.config.features_path):\n",
    "            raise FileNotFoundError(f\"Features file not found: {self.config.features_path}\")\n",
    "        if not os.path.exists(self.config.model_path):\n",
    "            raise FileNotFoundError(f\"Model file not found: {self.config.model_path}\")\n",
    "        if not os.path.exists(self.config.label_encoder_path):\n",
    "            raise FileNotFoundError(f\"Label encoder file not found: {self.config.label_encoder_path}\")\n",
    "\n",
    "        X = np.load(self.config.features_path, allow_pickle=True).item()\n",
    "        y = np.load(self.config.labels_path, allow_pickle=True).item()\n",
    "        X = X['X'] \n",
    "        y = y['y']\n",
    "\n",
    "        model = joblib.load(self.config.model_path)\n",
    "        label_encoder = joblib.load(self.config.label_encoder_path)\n",
    "\n",
    "        logger.info(\"Loaded features, model, and label encoder.\")\n",
    "\n",
    "        # Split data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y,\n",
    "            test_size=self.config.test_size,\n",
    "            random_state=self.config.random_state,\n",
    "            stratify=y\n",
    "        )\n",
    "\n",
    "        logger.info(\"Split data into train and test sets.\")\n",
    "\n",
    "        # Predictions and metrics\n",
    "        preds = model.predict(X_test)\n",
    "\n",
    "        metrics = {\n",
    "            \"accuracy\": accuracy_score(y_test, preds),\n",
    "            \"precision_weighted\": precision_score(y_test, preds, average=\"weighted\"),\n",
    "            \"recall_weighted\": recall_score(y_test, preds, average=\"weighted\"),\n",
    "            \"f1_weighted\": f1_score(y_test, preds, average=\"weighted\"),\n",
    "            \"precision_macro\": precision_score(y_test, preds, average=\"macro\"),\n",
    "            \"recall_macro\": recall_score(y_test, preds, average=\"macro\"),\n",
    "            \"f1_macro\": f1_score(y_test, preds, average=\"macro\"),\n",
    "        }\n",
    "\n",
    "        create_directories([os.path.dirname(self.config.metrics_path)])\n",
    "        save_json(path=Path(self.config.metrics_path), data=metrics)\n",
    "        logger.info(f\"Metrics saved to {self.config.metrics_path}\")\n",
    "\n",
    "        # MLflow logging\n",
    "        with mlflow.start_run(run_name=\"Model Evaluation\"):\n",
    "            mlflow.log_metrics({k: float(v) for k, v in metrics.items()})\n",
    "            mlflow.set_tag(\"stage\", \"evaluation\")\n",
    "            mlflow.set_tag(\"evaluation_date\", str(pd.Timestamp.now()))\n",
    "            mlflow.log_artifact(self.config.metrics_path)\n",
    "            mlflow.log_artifact(self.config.model_path)\n",
    "            mlflow.log_artifact(self.config.label_encoder_path)\n",
    "            logger.info(\"Metrics and artifacts logged to MLflow.\")\n",
    "\n",
    "            cm = confusion_matrix(y_test, preds)\n",
    "            plt.figure(figsize=(6, 4))\n",
    "            sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "            plt.title(\"Confusion Matrix\")\n",
    "            plt.xlabel(\"Predicted\")\n",
    "            plt.ylabel(\"Actual\")\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            cm_save_path = os.path.join(os.path.dirname(self.config.cm_path), \"cm.png\")\n",
    "            create_directories([os.path.dirname(cm_save_path)])\n",
    "            plt.savefig(cm_save_path)\n",
    "            plt.close()\n",
    "            mlflow.log_artifact(cm_save_path)\n",
    "            logger.info(f\"Confusion matrix saved to {cm_save_path}\")\n",
    "\n",
    "        logger.info(\"Model evaluation complete. Metrics and plots logged.\")\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc74a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-20 20:19:46,878: INFO: helpers: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-06-20 20:19:46,884: INFO: helpers: yaml file: config\\schema.yaml loaded successfully]\n",
      "[2025-06-20 20:19:46,884: INFO: helpers: yaml file: config\\params.yaml loaded successfully]\n",
      "[2025-06-20 20:19:46,900: INFO: helpers: created directory at: artifacts]\n",
      "[2025-06-20 20:19:46,900: INFO: helpers: created directory at: artifacts/model_evaluation]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-06-20 20:19:47,717: INFO: helpers: Repository initialized!]\n",
      "[2025-06-20 20:19:49,850: INFO: 1549089019: MLflow tracking setup complete.]\n",
      "[2025-06-20 20:19:52,473: INFO: 1549089019: Loaded features, model, and label encoder.]\n",
      "[2025-06-20 20:19:52,934: INFO: 1549089019: Split data into train and test sets.]\n",
      "[2025-06-20 20:19:53,667: INFO: helpers: created directory at: artifacts/model_evaluation]\n",
      "[2025-06-20 20:19:53,686: INFO: helpers: json file saved at: artifacts\\model_evaluation\\metrics.json]\n",
      "[2025-06-20 20:19:53,689: INFO: 1549089019: Metrics saved to artifacts/model_evaluation/metrics.json]\n",
      "[2025-06-20 20:19:54,633: INFO: 1549089019: Metrics and artifacts logged to MLflow.]\n",
      "[2025-06-20 20:19:55,888: INFO: helpers: created directory at: artifacts/model_evaluation]\n",
      "[2025-06-20 20:19:56,499: INFO: 1549089019: Confusion matrix saved to artifacts/model_evaluation\\cm.png]\n",
      "üèÉ View run Model Evaluation at: http://localhost:5000/#/experiments/374096542705668379/runs/7e850456435543f99f3462d57bd1d289\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/374096542705668379\n",
      "[2025-06-20 20:19:56,572: INFO: 1549089019: Model evaluation complete. Metrics and plots logged.]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluator(config=model_evaluation_config)\n",
    "    model_evaluation.evaluation()\n",
    "        \n",
    "except FileNotFoundError as e:\n",
    "    raise CustomException(str(e), sys)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Condition2Cure",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
