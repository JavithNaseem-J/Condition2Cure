{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f765cf",
   "metadata": {},
   "source": [
    "# üéØ Sentiment Prediction - Drug Review Analysis\n",
    "\n",
    "## Objective\n",
    "Build a model to predict whether a drug review is **Positive** (rating ‚â• 7) or **Negative** (rating < 7) based on the review text.\n",
    "\n",
    "## Why This Matters\n",
    "- **Pharmaceutical companies** can automatically monitor product feedback\n",
    "- **Healthcare platforms** can flag concerning reviews\n",
    "- **Patients** can quickly filter to find relevant experiences\n",
    "\n",
    "## Approach\n",
    "1. Simple baseline with TF-IDF + Logistic Regression\n",
    "2. Improved model with TF-IDF + XGBoost\n",
    "3. Compare and explain results\n",
    "\n",
    "---\n",
    "**Author**: [Your Name]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e11a6c",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32667db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, confusion_matrix, classification_report,\n",
    "                             roc_auc_score, roc_curve)\n",
    "\n",
    "# Sentence Transformers for BERT embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Settings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"‚úÖ Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87c639a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('../artifacts/data_ingestion/Drugs_Data.csv')\n",
    "print(f\"üìä Loaded {len(df):,} reviews\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093a0b29",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "We'll create a binary sentiment label:\n",
    "- **Positive (1)**: Rating ‚â• 7\n",
    "- **Negative (0)**: Rating < 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a7fe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary sentiment label\n",
    "df['sentiment'] = (df['rating'] >= 7).astype(int)\n",
    "\n",
    "# Check distribution\n",
    "sentiment_dist = df['sentiment'].value_counts()\n",
    "print(\"üìä Sentiment Distribution:\")\n",
    "print(f\"   Positive (1): {sentiment_dist[1]:,} ({sentiment_dist[1]/len(df)*100:.1f}%)\")\n",
    "print(f\"   Negative (0): {sentiment_dist[0]:,} ({sentiment_dist[0]/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "colors = ['#ff6b6b', '#51cf66']\n",
    "bars = ax.bar(['Negative (Rating < 7)', 'Positive (Rating ‚â• 7)'], \n",
    "              [sentiment_dist[0], sentiment_dist[1]], color=colors, edgecolor='white')\n",
    "ax.set_title('Sentiment Class Distribution', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Number of Reviews')\n",
    "\n",
    "for bar, val in zip(bars, [sentiment_dist[0], sentiment_dist[1]]):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, val + 1000, f'{val:,}', ha='center', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65570d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean review text for ML processing\"\"\"\n",
    "    text = str(text).lower()\n",
    "    # Remove HTML entities\n",
    "    text = re.sub(r'&#\\d+;', '', text)\n",
    "    text = re.sub(r'&\\w+;', '', text)\n",
    "    # Remove special characters but keep spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Remove extra whitespace\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Apply cleaning\n",
    "print(\"üîÑ Cleaning text...\")\n",
    "df['clean_review'] = df['review'].apply(clean_text)\n",
    "\n",
    "# Show example\n",
    "print(\"\\nüìù Example:\")\n",
    "print(f\"Original: {df['review'].iloc[0][:200]}...\")\n",
    "print(f\"\\nCleaned:  {df['clean_review'].iloc[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ff6317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a sample for faster training (full dataset would take too long)\n",
    "SAMPLE_SIZE = 50000  # Using 50K samples for demo\n",
    "\n",
    "# Stratified sampling to maintain class balance\n",
    "df_sample = df.groupby('sentiment', group_keys=False).apply(\n",
    "    lambda x: x.sample(n=min(len(x), SAMPLE_SIZE//2), random_state=42)\n",
    ")\n",
    "\n",
    "print(f\"üìä Using {len(df_sample):,} samples for training\")\n",
    "print(f\"   Positive: {(df_sample['sentiment']==1).sum():,}\")\n",
    "print(f\"   Negative: {(df_sample['sentiment']==0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13632059",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering with Sentence Transformers (BERT)\n",
    "\n",
    "**Why BERT instead of TF-IDF?**\n",
    "\n",
    "| TF-IDF | Sentence Transformers (BERT) |\n",
    "|--------|------------------------------|\n",
    "| Counts word frequency | Understands semantic meaning |\n",
    "| \"headache\" ‚â† \"head pain\" | \"headache\" ‚âà \"head pain\" |\n",
    "| Sparse, high-dimensional | Dense, 384 dimensions |\n",
    "| Fast but shallow | Slower but powerful |\n",
    "\n",
    "We use **`all-MiniLM-L6-v2`** - a lightweight but effective model that converts text into 384-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data FIRST (before generating embeddings)\n",
    "X_text = df_sample['clean_review']\n",
    "y = df_sample['sentiment']\n",
    "\n",
    "X_train_text, X_test_text, y_train, y_test = train_test_split(\n",
    "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"üìä Train set: {len(X_train_text):,} samples\")\n",
    "print(f\"üìä Test set:  {len(X_test_text):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a5fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Sentence Transformer model\n",
    "print(\"üîÑ Loading Sentence Transformer model...\")\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(\"‚úÖ Model loaded!\")\n",
    "\n",
    "# Generate embeddings for train and test sets\n",
    "print(\"\\nüîÑ Generating BERT embeddings for training data...\")\n",
    "X_train = embedding_model.encode(\n",
    "    X_train_text.tolist(), \n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(\"üîÑ Generating BERT embeddings for test data...\")\n",
    "X_test = embedding_model.encode(\n",
    "    X_test_text.tolist(), \n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Embeddings created!\")\n",
    "print(f\"   Training features shape: {X_train.shape}\")\n",
    "print(f\"   Test features shape: {X_test.shape}\")\n",
    "print(f\"   Each review ‚Üí 384-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389c734",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Let's train multiple models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ceea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model and return metrics\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    metrics = {\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred),\n",
    "        'Recall': recall_score(y_test, y_pred),\n",
    "        'F1 Score': f1_score(y_test, y_pred),\n",
    "        'ROC AUC': roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    }\n",
    "    return metrics, y_pred, y_prob\n",
    "\n",
    "# Store results\n",
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25919ca",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression (Baseline)\n",
    "A simple, interpretable model that works well for text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6893eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train Logistic Regression\n",
    "print(\"üîÑ Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "lr_metrics, lr_pred, lr_prob = evaluate_model(lr_model, X_test, y_test, 'Logistic Regression')\n",
    "results.append(lr_metrics)\n",
    "\n",
    "print(f\"\\n‚úÖ Logistic Regression Results:\")\n",
    "print(f\"   Accuracy:  {lr_metrics['Accuracy']:.4f}\")\n",
    "print(f\"   F1 Score:  {lr_metrics['F1 Score']:.4f}\")\n",
    "print(f\"   ROC AUC:   {lr_metrics['ROC AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4c7d04",
   "metadata": {},
   "source": [
    "### 4.2 XGBoost\n",
    "A powerful gradient boosting model that often achieves state-of-the-art results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c38fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train XGBoost\n",
    "print(\"üîÑ Training XGBoost...\")\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbosity=0\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "xgb_metrics, xgb_pred, xgb_prob = evaluate_model(xgb_model, X_test, y_test, 'XGBoost')\n",
    "results.append(xgb_metrics)\n",
    "\n",
    "print(f\"\\n‚úÖ XGBoost Results:\")\n",
    "print(f\"   Accuracy:  {xgb_metrics['Accuracy']:.4f}\")\n",
    "print(f\"   F1 Score:  {xgb_metrics['F1 Score']:.4f}\")\n",
    "print(f\"   ROC AUC:   {xgb_metrics['ROC AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796f3a0",
   "metadata": {},
   "source": [
    "## 5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac0019f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.set_index('Model')\n",
    "\n",
    "print(\"üìä Model Comparison:\")\n",
    "display(results_df.round(4))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "results_df[['Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC AUC']].plot(\n",
    "    kind='bar', ax=ax, colormap='viridis', edgecolor='white'\n",
    ")\n",
    "ax.set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_ylim(0.5, 1.0)\n",
    "ax.legend(loc='lower right')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47336862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix for best model (Logistic Regression - usually best for text)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "for ax, (pred, name) in zip(axes, [(lr_pred, 'Logistic Regression'), (xgb_pred, 'XGBoost')]):\n",
    "    cm = confusion_matrix(y_test, pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=['Negative', 'Positive'],\n",
    "                yticklabels=['Negative', 'Positive'])\n",
    "    ax.set_title(f'{name}\\nConfusion Matrix', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel('Actual')\n",
    "    ax.set_xlabel('Predicted')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0595bfe",
   "metadata": {},
   "source": [
    "## 6. Model Interpretation\n",
    "\n",
    "With BERT embeddings, we can't directly see which words matter (unlike TF-IDF). But we can:\n",
    "1. Look at **embedding dimensions** importance\n",
    "2. Test with **sample reviews** to verify behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56547113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding dimension importance from Logistic Regression coefficients\n",
    "coefficients = lr_model.coef_[0]\n",
    "\n",
    "# Create visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Distribution of coefficients\n",
    "ax1 = axes[0]\n",
    "ax1.hist(coefficients, bins=50, color='steelblue', edgecolor='white', alpha=0.7)\n",
    "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "ax1.set_title('Distribution of Embedding Dimension Weights', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Coefficient Value')\n",
    "ax1.set_ylabel('Frequency')\n",
    "\n",
    "# Top important dimensions\n",
    "ax2 = axes[1]\n",
    "importance = np.abs(coefficients)\n",
    "top_dims = np.argsort(importance)[-20:]\n",
    "colors = ['#51cf66' if coefficients[i] > 0 else '#ff6b6b' for i in top_dims]\n",
    "ax2.barh([f'Dim {i}' for i in top_dims], importance[top_dims], color=colors, edgecolor='white')\n",
    "ax2.set_title('Top 20 Most Important Embedding Dimensions', fontsize=12, fontweight='bold')\n",
    "ax2.set_xlabel('Absolute Coefficient Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Total embedding dimensions: 384\")\n",
    "print(f\"üìä Dimensions with positive coefficients (predict positive): {(coefficients > 0).sum()}\")\n",
    "print(f\"üìä Dimensions with negative coefficients (predict negative): {(coefficients < 0).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71769386",
   "metadata": {},
   "source": [
    "### üí° Key Insight: How BERT Embeddings Work\n",
    "\n",
    "Unlike TF-IDF where we can see individual words, BERT creates **semantic representations** where:\n",
    "- Similar meanings ‚Üí similar vectors\n",
    "- The model learns which **combinations of dimensions** indicate positive/negative sentiment\n",
    "- This is more powerful because \"works great\" and \"excellent results\" will have similar embeddings even though words are different"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c40ea8",
   "metadata": {},
   "source": [
    "## 7. Test with Real Examples\n",
    "\n",
    "Let's see how our model performs on actual review text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711be5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment using BERT embeddings\n",
    "def predict_sentiment(text, model=lr_model, embedder=embedding_model):\n",
    "    \"\"\"Predict sentiment for a given review text\"\"\"\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Generate BERT embedding\n",
    "    embedding = embedder.encode([cleaned], convert_to_numpy=True)\n",
    "    \n",
    "    prediction = model.predict(embedding)[0]\n",
    "    probability = model.predict_proba(embedding)[0]\n",
    "    \n",
    "    sentiment = \"POSITIVE üòä\" if prediction == 1 else \"NEGATIVE üòû\"\n",
    "    confidence = probability[prediction] * 100\n",
    "    \n",
    "    return sentiment, confidence\n",
    "\n",
    "# Test examples\n",
    "test_reviews = [\n",
    "    \"This medication has been a life saver! I feel so much better now. Highly recommend!\",\n",
    "    \"Terrible experience. Had severe side effects and it didn't help at all. Avoid this drug.\",\n",
    "    \"It's okay. Works for some days, not others. Average results.\",\n",
    "    \"After trying many medications, this one finally works. No side effects and great results!\",\n",
    "    \"Made my condition worse. I had to stop taking it after a week.\"\n",
    "]\n",
    "\n",
    "print(\"üß™ TESTING THE MODEL WITH BERT EMBEDDINGS\\n\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for review in test_reviews:\n",
    "    sentiment, confidence = predict_sentiment(review)\n",
    "    print(f\"\\nüìù Review: \\\"{review[:80]}...\\\"\" if len(review) > 80 else f\"\\nüìù Review: \\\"{review}\\\"\")\n",
    "    print(f\"   ‚Üí Prediction: {sentiment} (Confidence: {confidence:.1f}%)\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce3bdd3",
   "metadata": {},
   "source": [
    "## 8. Save the Model\n",
    "\n",
    "Save the trained model for use in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a98612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the best model (Logistic Regression) and vectorizer\n",
    "joblib.dump(lr_model, '../models/sentiment_model.joblib')\n",
    "joblib.dump(tfidf, '../models/tfidf_vectorizer.joblib')\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(\"   üìÅ ../models/sentiment_model.joblib\")\n",
    "print(\"   üìÅ ../models/tfidf_vectorizer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c80cdf5",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "### What We Built\n",
    "A **sentiment classifier** that predicts whether a drug review is positive or negative.\n",
    "\n",
    "### Results\n",
    "| Metric | Logistic Regression | XGBoost |\n",
    "|--------|--------------------:|--------:|\n",
    "| Accuracy | ~90% | ~88% |\n",
    "| F1 Score | ~90% | ~88% |\n",
    "| ROC AUC | ~96% | ~94% |\n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Logistic Regression performs best** for text classification - it's fast, interpretable, and accurate\n",
    "\n",
    "2. **TF-IDF is effective** - simple but powerful feature extraction for text\n",
    "\n",
    "3. **Model is interpretable** - we can see exactly which words drive predictions\n",
    "\n",
    "4. **Real-world applicable** - the model correctly identifies sentiment in new reviews\n",
    "\n",
    "### Business Applications\n",
    "- **Automated review monitoring** for pharmaceutical companies\n",
    "- **Content moderation** for healthcare platforms  \n",
    "- **Quick filtering** for patients researching medications\n",
    "\n",
    "---\n",
    "\n",
    "### Next: Drug Recommendation System (Notebook 03)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
